{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDM8-zIUsNKW"
   },
   "source": [
    "## CS 644 - Final Project \n",
    "\n",
    "##### Group Members :- Smeet Kathiria "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jQjpnTxwuSl"
   },
   "source": [
    "# Project Part 2 -> Data Processing and Model Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqXPuuenrrDB"
   },
   "source": [
    "##### Setting up pyspark  environment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8O9WGoY8reqO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/spark2.4.3\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/local/anaconda/bin/python\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/local/anaconda/bin/python\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVP3DyTxruzh"
   },
   "source": [
    "##### loading all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtWpEy61reqV"
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "from pyspark.sql import SparkSession,DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql import SQLContext\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import NGram, VectorAssembler\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jc_9ESfQsv78"
   },
   "source": [
    "##### Creating spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDeLtt8-reqW"
   },
   "outputs": [],
   "source": [
    "sc = SparkSession.builder.appName(\"modelbuild\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgmR22Ges1aA"
   },
   "source": [
    "##### Loading Twitter Training Data stored in Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lM7ZDNpEreqX"
   },
   "outputs": [],
   "source": [
    "TRAININGDATA_PATH = \"/user/smeetp269618/twitter_training.csv\"\n",
    "df = sc.read.csv(\n",
    "    TRAININGDATA_PATH,\n",
    "    inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kC1Y0ZjireqX",
    "outputId": "dde15880-a109-4f25-9246-74f434378447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records in df :  1600000\n"
     ]
    }
   ],
   "source": [
    "print('Total Number of records in df : ',df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-WUksqYreqY",
    "outputId": "dbd020de-c096-40a4-e690-21baa984cc93",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cgQAvEItBI8"
   },
   "source": [
    "##### Renaming columns for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0lpx2jKreqY",
    "outputId": "e126ba20-2042-4721-d6b5-ba863e777509",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(text,StringType,true),StructField(sentiment,IntegerType,true)))"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumnRenamed('_c0','sentiment').withColumnRenamed('_c5','text')\n",
    "df.select('text','sentiment').schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgK6uBIvtGY4"
   },
   "source": [
    "##### Transofrming the tweets based on analysis conducted in the Twitter data analysis notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyd2gJrcreqZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "negations_catalog = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "\n",
    "combined_regex = r'|'.join((r'@[A-Za-z0-9_]+', r'https?://[^ ]+'))\n",
    "\n",
    "neg_regex = re.compile(r'\\b(' + '|'.join(negations_catalog.keys()) + r')\\b')\n",
    "\n",
    "def tweets_transformer(line):\n",
    "    text = line.text\n",
    "    strip_text = re.sub(combined_regex, '', text)\n",
    "    strip_text = re.sub(r'www.[^ ]+', '', strip_text)\n",
    "    lower_case = strip_text.lower()\n",
    "    neg_transform = neg_regex.sub(lambda x: negations_catalog[x.group()], lower_case)\n",
    "    letters_filter = re.sub(\"[^a-zA-Z]\", \" \", neg_transform)\n",
    "    normal_spaced = re.sub(' +',' ',letters_filter)\n",
    "    return normal_spaced, line.sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_O218edreqa"
   },
   "outputs": [],
   "source": [
    "cleaned_rdd = df.rdd.map(tweets_transformer)\n",
    "cleaned_df = cleaned_rdd.toDF([\"text\",\"sentiment\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIHyiaR8reqa",
    "outputId": "53399a82-ecc6-47be-dcf1-070e0df7015f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[text: string, sentiment: bigint]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xuhWVBIreqb",
    "outputId": "363906d2-9937-4897-99dc-78f50f87f08f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                text|sentiment|\n",
      "+--------------------+---------+\n",
      "| awww that s a bu...|        0|\n",
      "|is upset that he ...|        0|\n",
      "| i dived many tim...|        0|\n",
      "|my whole body fee...|        0|\n",
      "| no it s not beha...|        0|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_df.select('text','sentiment').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLboadgCreqb",
    "outputId": "019d82cf-8b4b-4ca3-aee6-e636baa6666d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                text|sentiment|\n",
      "+--------------------+---------+\n",
      "| awww that s a bu...|        0|\n",
      "|is upset that he ...|        0|\n",
      "| i dived many tim...|        0|\n",
      "| no it s not beha...|        0|\n",
      "| hey long time no...|        0|\n",
      "| i could not bear...|        0|\n",
      "| it it counts idk...|        0|\n",
      "| i would ve been ...|        0|\n",
      "| i wish i got to ...|        0|\n",
      "|hollis death scen...|        0|\n",
      "| ahh ive always w...|        0|\n",
      "| oh dear were you...|        0|\n",
      "| i was out most o...|        0|\n",
      "|one of my friend ...|        0|\n",
      "|just going to cry...|        0|\n",
      "|ooooh lol that le...|        0|\n",
      "|meh almost lover ...|        0|\n",
      "|some hacked my ac...|        0|\n",
      "| i want to go to ...|        0|\n",
      "|thought sleeping ...|        0|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_df.createOrReplaceTempView(\"cleandf\")\n",
    "final_df = sc.sql(\"SELECT * FROM cleandf WHERE LENGTH(text) > 50\")\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6T11k9lgreqc",
    "outputId": "53e9f56a-7832-4065-de25-ee0dab66bf4d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|sentiment|\n",
      "+---------+\n",
      "|        0|\n",
      "|        4|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.select('sentiment').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5vx3ANYreqc",
    "outputId": "c4ed4baa-b902-4e53-97c9-7b1da7332c70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|sentiment| count|\n",
      "+---------+------+\n",
      "|        0|483666|\n",
      "|        4|451968|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.groupby('sentiment').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmhXYn6ntaKp"
   },
   "source": [
    "##### Changing all the values where sentiment is 4 to 1 just for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRZkJQ_creqc"
   },
   "outputs": [],
   "source": [
    "def sentimentMapper(sentiment):\n",
    "    return 1 if sentiment == 4 else sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oA8zVDpfreqd",
    "outputId": "3a19cbfd-8a42-4e37-86f0-64fcb34212cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                text|sentiment|\n",
      "+--------------------+---------+\n",
      "| awww that s a bu...|        0|\n",
      "|is upset that he ...|        0|\n",
      "| i dived many tim...|        0|\n",
      "| no it s not beha...|        0|\n",
      "| hey long time no...|        0|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfSentimentMapper = udf(sentimentMapper, IntegerType())\n",
    "final_df = final_df.withColumn('sentiment', udfSentimentMapper('sentiment'))\n",
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0e04YJuhreqd",
    "outputId": "5ebbe784-d1da-4fec-e525-a2f7b86affd1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|sentiment| count|\n",
      "+---------+------+\n",
      "|        1|451968|\n",
      "|        0|483666|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.groupby('sentiment').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEr4PaPsreqd",
    "outputId": "4761b61f-dbfe-4cb5-916d-ea7e98153d2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(text,StringType,true),StructField(sentiment,IntegerType,true)))"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J16J7ux9tmOw"
   },
   "source": [
    "<font color='#065535'>Selecting text and sentiment columns for model training</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5-CjxJ6reqd",
    "outputId": "c746e287-2293-4ca3-88f5-fc7021a37f9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                text|sentiment|\n",
      "+--------------------+---------+\n",
      "| awww that s a bu...|        0|\n",
      "|is upset that he ...|        0|\n",
      "| i dived many tim...|        0|\n",
      "| no it s not beha...|        0|\n",
      "| hey long time no...|        0|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df=final_df.select('text','sentiment')\n",
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NC9OFLZSreqe"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover,Word2Vec,StringIndexer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIfC_Qfstt2_"
   },
   "source": [
    "##### Splitting the data into train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yt-Yszjsreqe"
   },
   "outputs": [],
   "source": [
    "(train_set, val_set, test_set) = final_df.randomSplit([0.98, 0.01, 0.01], seed = 2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehzDmOvbtyYL"
   },
   "source": [
    "##### Building ML model using Tokenizer,ngrams, Count Vectorizer IDF,Vector Aassembler, label string indexer, Logistic regression. \n",
    "\n",
    "##### We used logistic regression since it performs better in classification tasks \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2OtPe_3reqe"
   },
   "outputs": [],
   "source": [
    "def ngrams_builder(inputCol=[\"text\",\"sentiment\"], n=3):\n",
    "    \n",
    "    tokenizer = [Tokenizer(inputCol=\"text\", \n",
    "                           outputCol=\"words\")]\n",
    "    \n",
    "    ngrams = [\n",
    "        NGram(n=i, \n",
    "              inputCol=\"words\", \n",
    "              outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    countVectorizer = [\n",
    "        CountVectorizer(vocabSize=7260,\n",
    "        inputCol=\"{0}_grams\".format(i),\n",
    "        outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    \n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), \n",
    "            outputCol=\"{0}_tfidf\".format(i), \n",
    "            minDocFreq=5\n",
    "              ) \n",
    "           for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"features\"\n",
    "    )]\n",
    "    \n",
    "    label_stringIndexer = [StringIndexer(inputCol = \"sentiment\", \n",
    "                                         outputCol = \"label\")]\n",
    "    \n",
    "    lr = [LogisticRegression(maxIter=100)]\n",
    "    \n",
    "    return Pipeline(stages = tokenizer + ngrams + countVectorizer + idf + assembler + label_stringIndexer + lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqnktcDXv-0K"
   },
   "source": [
    "##### Doing training and predictions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rg-Qdyzhreqf",
    "outputId": "490c0371-02f5-4327-eb49-82d4e7d2bc56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8037\n"
     ]
    }
   ],
   "source": [
    "pipeline = ngrams_builder()\n",
    "\n",
    "ngram_model = pipeline.fit(train_set)\n",
    "\n",
    "predictions = ngram_model.transform(val_set)\n",
    "\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psoW0FdjwPA9"
   },
   "source": [
    "##### Saving the model which we will use to predict on the live streaming data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsVDYYIFreqf"
   },
   "outputs": [],
   "source": [
    "ngram_model.save(\"twitter_sentiment_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP7qDGEewVoL"
   },
   "source": [
    "##### Checking some predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmOmo8nDreqf",
    "outputId": "3bc60033-d772-4f74-8514-8988144269da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                text|prediction|\n",
      "+--------------------+----------+\n",
      "| again i have no ...|       0.0|\n",
      "| ahaha i m actual...|       1.0|\n",
      "| ahh i knowww but...|       0.0|\n",
      "| all s fine in do...|       0.0|\n",
      "| ally thanks for ...|       0.0|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('text','prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tECJp4Utreqg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRRAw6Tereqg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pes-oUqSreqg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7_NhdOXreqg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cy7yiuRBreqg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFYmbbHzreqg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project Part 2 -> Data Processing and Model Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
